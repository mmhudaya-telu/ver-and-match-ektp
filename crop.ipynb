{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confirmed-civilian",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "australian-strip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cu110 True\n"
     ]
    }
   ],
   "source": [
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Detectron\n",
    "import detectron2\n",
    "\n",
    "# Helper Library\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "working-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-canadian",
   "metadata": {},
   "source": [
    "##### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "perceived-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_model_in_folder(folder_dir):\n",
    "    for root, dirs, files in os.walk(folder_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".pth\") and file.startswith(\"model\"):\n",
    "                path = os.path.join(root, file)\n",
    "                path = path.replace(\"\\\\\", \"/\")\n",
    "                print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-sherman",
   "metadata": {},
   "source": [
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "entire-bloom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/faster-rcnn/05.04.2021, 22;28;07/COCO-Detection/faster_rcnn_R_50_C4_1x.yaml/model_final.pth\n",
      "./results/faster-rcnn/05.05.2021, 04;58;13/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml/model_final.pth\n",
      "./results/faster-rcnn/05.05.2021, 05;20;20/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml/model_final.pth\n",
      "./results/faster-rcnn/05.22.2021, 13;06;41/COCO-Detection/faster_rcnn_R_50_C4_1x.yaml/model_final.pth\n",
      "./results/faster-rcnn/05.22.2021, 13;19;25/COCO-Detection/faster_rcnn_R_50_C4_1x.yaml/model_final.pth\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "find_all_model_in_folder('./results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spanish-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultTrainer\n",
    "## Validation\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "\n",
    "\n",
    "cfg_zoo = \"faster_rcnn_R_50_C4_1x.yaml\"\n",
    "cfg_model_path = \"./results/faster-rcnn/05.04.2021, 22;28;07/COCO-Detection/faster_rcnn_R_50_C4_1x.yaml/model_final.pth\"\n",
    "\n",
    "# Initialize predictor\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/\" + cfg_zoo))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.WEIGHTS = cfg_model_path\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-restaurant",
   "metadata": {},
   "source": [
    "#### Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "contrary-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_all_img(folder):\n",
    "    images_dict = {}\n",
    "    for filename in os.listdir(folder):\n",
    "        filename_number = filename.split('_')[0]\n",
    "        filename_type = filename.split('_')[1]\n",
    "        if filename_number not in images_dict:\n",
    "            images_dict[filename_number] = {}\n",
    "        \n",
    "        if filename_type.find(\"selfie\") == -1:\n",
    "            # e-ktp image\n",
    "            images_dict[filename_number]['card'] = {\n",
    "                'path': os.path.join(folder,filename),\n",
    "                'bbox': None,\n",
    "                'format': filename.split('.')[1]\n",
    "            }\n",
    "        else:\n",
    "            images_dict[filename_number]['selfie'] = {\n",
    "                'path': os.path.join(folder,filename),\n",
    "                'bbox': None,\n",
    "                'format': filename.split('.')[1]\n",
    "            }\n",
    "        \n",
    "    return images_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alpha-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it should take a while since we put all the images into phython cache\n",
    "train_dir = \"./dataset/e-ktp/train/images\"\n",
    "val_dir = \"./dataset/e-ktp/val/images\"\n",
    "train_dict = get_dict_all_img(train_dir)\n",
    "val_dict = get_dict_all_img(val_dir)\n",
    "\n",
    "images_dict = dict(train_dict, **val_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-railway",
   "metadata": {},
   "source": [
    "#### Remove Background and Verification (Detect)\n",
    "We will try to use a GrabCut and probably there's some images that heavily blurred and the algorithm will not taking the actual foreground of the images. But this will be a room of improvement in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sealed-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing background on selfie images\n",
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mechanical-locking",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\project\\skripsi\\libs\\detectron2\\detectron2\\modeling\\roi_heads\\fast_rcnn.py:154: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  filter_inds = filter_mask.nonzero()\n"
     ]
    }
   ],
   "source": [
    "# Somehow adding this line make the predictor of the bounding boxes not error\n",
    "# Without this lines, the predictor on cell below will got exploded by memory leak\n",
    "im = cv2.imread(images_dict['7'][\"selfie\"][\"path\"])\n",
    "outputs = predictor(im)\n",
    "images_dict['7'][\"selfie\"][\"bbox\"] = outputs[\"instances\"].to(\"cpu\").pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "enabling-killer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# localize all e-ktp position\n",
    "for image_key in images_dict:\n",
    "    # Localize selfie image\n",
    "    im = cv2.imread(images_dict[image_key][\"selfie\"][\"path\"])\n",
    "    outputs = predictor(im)\n",
    "    images_dict[image_key][\"selfie\"][\"bbox\"] = outputs[\"instances\"].to(\"cpu\").pred_boxes\n",
    "\n",
    "    # Localize card image\n",
    "    im_ = cv2.imread(images_dict[image_key][\"card\"][\"path\"])\n",
    "    outputs_ = predictor(im_)\n",
    "    images_dict[image_key][\"card\"][\"bbox\"] = outputs_[\"instances\"].to(\"cpu\").pred_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-christopher",
   "metadata": {},
   "source": [
    "#### Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "appreciated-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop function\n",
    "def get_cropped_imgs_bbox(img_path, bboxes):\n",
    "    images = []\n",
    "    bbox_arr = bboxes.tensor.cpu().numpy()\n",
    "    im = cv2.imread(img_path)\n",
    "    \n",
    "    for bbox in bbox_arr:\n",
    "        x0 = np.around(bbox[0]).astype(int) \n",
    "        y0 = np.around(bbox[1]).astype(int)\n",
    "        x1 = np.around(bbox[2]).astype(int)\n",
    "        y1 = np.around(bbox[3]).astype(int)\n",
    "        \n",
    "        images.append(im[y0:y1,x0:x1])\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cathedral-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset crop folder\n",
    "crop_path = \"./results/cropped/e-ktp\"\n",
    "\n",
    "# Delete previous cropped image\n",
    "for filename in os.listdir(crop_path):\n",
    "    file_path = os.path.join(crop_path, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "            os.unlink(file_path)\n",
    "        elif os.path.isdir(file_path):\n",
    "            shutil.rmtree(file_path)\n",
    "    except Exception as e:\n",
    "        print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acceptable-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "for image_key in images_dict:\n",
    "    # Save cropped selfie results\n",
    "    img_selfie_obj = images_dict[image_key][\"selfie\"]\n",
    "    cropped_selfie_images = get_cropped_imgs_bbox(img_selfie_obj[\"path\"], img_selfie_obj[\"bbox\"])\n",
    "    for idx, cropped_selfie_image in enumerate(cropped_selfie_images):\n",
    "        imageRGB = cv2.cvtColor(cropped_selfie_image, cv2.COLOR_BGR2RGB)\n",
    "        im = Image.fromarray(imageRGB)\n",
    "        im.save(crop_path+\"/\"+image_key+\"_selfie_crop_\"+str(idx)+\".\"+img_selfie_obj[\"format\"])\n",
    "    \n",
    "    # Save cropped card results\n",
    "    img_card_obj = images_dict[image_key][\"card\"]\n",
    "    cropped_card_images = get_cropped_imgs_bbox(img_card_obj[\"path\"], img_card_obj[\"bbox\"])\n",
    "    for idx, cropped_card_image in enumerate(cropped_card_images):\n",
    "        imageRGB = cv2.cvtColor(cropped_card_image, cv2.COLOR_BGR2RGB)\n",
    "        im = Image.fromarray(imageRGB)\n",
    "        im.save(crop_path+\"/\"+image_key+\"_card_crop_\"+str(idx)+\".\"+img_card_obj[\"format\"])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
