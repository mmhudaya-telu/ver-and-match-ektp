{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "median-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1+cu110 True\n",
      "=================RESULTS=================\n",
      "\n",
      "====DETECTION USING FASTER-RCNN====\n",
      "Total Cropped Image from Image 1: 2\n",
      "Total Cropped Image from Image 2: 2\n",
      "Execution Time: 2.5054636001586914\n",
      "===================================\n",
      "\n",
      "========MATCHING USING ORB========\n",
      "--Image 1 Cropped 0 to Image 2 Cropped 0\n",
      "Image 1 Total Keypoints: 2596\n",
      "Image 1 Dimensions: (572, 1023, 3)\n",
      "Image 2 Total Keypoints: 686\n",
      "Image 2 Dimensions: (633, 1023, 3)\n",
      "Total Matches: 377\n",
      "Total \"Good\" Matches: 357\n",
      "Execution Time: 0.0659644603729248 seconds\n",
      "----------------------------------------------\n",
      "--Image 1 Cropped 0 to Image 2 Cropped 1\n",
      "Image 1 Total Keypoints: 2594\n",
      "Image 1 Dimensions: (555, 992, 3)\n",
      "Image 2 Total Keypoints: 945\n",
      "Image 2 Dimensions: (386, 992, 3)\n",
      "Total Matches: 414\n",
      "Total \"Good\" Matches: 357\n",
      "Execution Time: 0.06499791145324707 seconds\n",
      "----------------------------------------------\n",
      "--Image 1 Cropped 1 to Image 2 Cropped 0\n",
      "Image 1 Total Keypoints: 2341\n",
      "Image 1 Dimensions: (1155, 1023, 3)\n",
      "Image 2 Total Keypoints: 686\n",
      "Image 2 Dimensions: (633, 1023, 3)\n",
      "Total Matches: 310\n",
      "Total \"Good\" Matches: 272\n",
      "Execution Time: 0.04300212860107422 seconds\n",
      "----------------------------------------------\n",
      "--Image 1 Cropped 1 to Image 2 Cropped 1\n",
      "Image 1 Total Keypoints: 2234\n",
      "Image 1 Dimensions: (1120, 992, 3)\n",
      "Image 2 Total Keypoints: 945\n",
      "Image 2 Dimensions: (386, 992, 3)\n",
      "Total Matches: 347\n",
      "Total \"Good\" Matches: 259\n",
      "Execution Time: 0.06500029563903809 seconds\n",
      "----------------------------------------------\n",
      "==================================\n",
      "\n",
      "Overall Execution Time: 2.7444283962249756 seconds\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "# Detectron\n",
    "import detectron2\n",
    "\n",
    "# Helper Library\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Training\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultTrainer\n",
    "## Validation\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "import time\n",
    "\n",
    "# Region cropping\n",
    "def get_predictor():\n",
    "    cfg_zoo = \"faster_rcnn_R_50_C4_1x.yaml\"\n",
    "    cfg_model_path = \"./results/faster-rcnn/05.04.2021, 22;28;07/COCO-Detection/faster_rcnn_R_50_C4_1x.yaml/model_final.pth\"\n",
    "\n",
    "    # Initialize predictor\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/\" + cfg_zoo))\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "    cfg.MODEL.WEIGHTS = cfg_model_path\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    return predictor\n",
    "\n",
    "def get_image_bounding_box(image_path, predictor):\n",
    "    im = cv2.imread(image_path)\n",
    "    outputs = predictor(im)\n",
    "    bbox = outputs[\"instances\"].to(\"cpu\").pred_boxes\n",
    "    return bbox\n",
    "\n",
    "def get_cropped_imgs_bbox(img_path, bboxes):\n",
    "    images = []\n",
    "    bbox_arr = bboxes.tensor.cpu().numpy()\n",
    "    im = cv2.imread(img_path)\n",
    "    \n",
    "    for bbox in bbox_arr:\n",
    "        x0 = np.around(bbox[0]).astype(int) \n",
    "        y0 = np.around(bbox[1]).astype(int)\n",
    "        x1 = np.around(bbox[2]).astype(int)\n",
    "        y1 = np.around(bbox[3]).astype(int)\n",
    "        \n",
    "        images.append(im[y0:y1,x0:x1])\n",
    "    \n",
    "    return images #bgr2gray later\n",
    "#endregion Cropping\n",
    "\n",
    "#Region Matching\n",
    "# Check smaller width dimension of 2 images\n",
    "# Smaller images: check width x dimension and choose the smaller\n",
    "# Resize higher image resolution with the smaller dimension (width / height)\n",
    "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation = inter)\n",
    "\n",
    "    # return the resized image\n",
    "    return resized\n",
    "\n",
    "def get_downscaled_images(img1, img2):\n",
    "    img1_h, img1_w, img1_c = img1.shape\n",
    "    img2_h, img2_w, img2_c = img2.shape\n",
    "    \n",
    "    if img1_h > img2_h:\n",
    "        scaled_img1 = image_resize(img1, width=img2_w)\n",
    "        scaled_img2 = img2\n",
    "    else:\n",
    "        scaled_img2 = image_resize(img2, width=img1_w)\n",
    "        scaled_img1 = img1\n",
    "        \n",
    "    \n",
    "    return scaled_img1, scaled_img2\n",
    "\n",
    "def get_orb_kp_and_des_by_img(img1):\n",
    "    n_features = 3000\n",
    "    factor = 2.0\n",
    "    orb = cv2.ORB_create(n_features, factor)\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    keypoints_img, descriptor_img =  orb.detectAndCompute(img1_gray, None)\n",
    "    return keypoints_img, descriptor_img\n",
    "\n",
    "def get_matches(query_orb_descriptor, train_orb_descriptor):\n",
    "    orb_bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    return orb_bf.match(query_orb_descriptor,train_orb_descriptor)\n",
    "\n",
    "# Set treshold to 64.0 based on ORB original paper ratio\n",
    "def get_good_orb_matches(matches):\n",
    "    good = []\n",
    "    for m in matches:\n",
    "        if m.distance < 64:\n",
    "            good.append(m)\n",
    "            \n",
    "    return good\n",
    "#endregion Matching\n",
    "\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    img1_path = './dataset/e-ktp/val/images/78_ktp.jpg' # for test\n",
    "    img2_path = './dataset/e-ktp/val/images/78_selfie.jpg' # for test\n",
    "    start_detection_time = time.time()\n",
    "    predictor = get_predictor()\n",
    "    img1_bbox = get_image_bounding_box(img1_path, predictor)\n",
    "    img2_bbox = get_image_bounding_box(img2_path, predictor)\n",
    "    end_detection_time = (time.time() - start_detection_time)\n",
    "    start_cropping_time = time.time()\n",
    "    imgs1_cropped = get_cropped_imgs_bbox(img1_path, img1_bbox)\n",
    "    imgs2_cropped = get_cropped_imgs_bbox(img2_path, img2_bbox)\n",
    "    end_cropping_time = (time.time() - start_cropping_time)\n",
    "    total_detection_time = end_detection_time + end_cropping_time\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx_img1, img1_cropped in enumerate(imgs1_cropped):\n",
    "        for idx_img2, img2_cropped in enumerate(imgs2_cropped):\n",
    "            each_time = time.time()\n",
    "            img1_ds, img2_ds = get_downscaled_images(img1_cropped, img2_cropped)\n",
    "            img1_kp, img1_desc = get_orb_kp_and_des_by_img(img1_ds)\n",
    "            img2_kp, img2_desc = get_orb_kp_and_des_by_img(img2_ds)\n",
    "            matches = get_matches(img1_desc, img2_desc)\n",
    "            good_matches = get_good_orb_matches(matches)\n",
    "            results.append({\n",
    "                \"compare\": \"Image 1 Cropped \"+str(idx_img1)+\" to Image 2 Cropped \"+str(idx_img2),\n",
    "                \"img1_keypoints\": len(img1_kp),\n",
    "                \"img1_dimension\": img1_ds.shape,\n",
    "                \"img2_keypoints\": len(img2_kp),\n",
    "                \"img2_dimension\": img2_ds.shape,\n",
    "                \"total_matches\": len(matches),\n",
    "                \"total_good_matches\": len(good_matches),\n",
    "                \"execution_time\": (time.time() - each_time)\n",
    "            })\n",
    "            \n",
    "    execution_time = (time.time() - start_time)\n",
    "    print(\"=================RESULTS=================\")\n",
    "    print()\n",
    "    print(\"====DETECTION USING FASTER-RCNN====\")\n",
    "    print(\"Total Cropped Image from Image 1: \"+str(len(imgs1_cropped)))\n",
    "    print(\"Total Cropped Image from Image 2: \"+str(len(imgs2_cropped)))\n",
    "    print(\"Execution Time: \"+str(end_cropping_time))\n",
    "    print(\"===================================\")\n",
    "    print()\n",
    "    print(\"========MATCHING USING ORB========\")\n",
    "    for result in results:\n",
    "        print(\"--\"+result['compare'])\n",
    "        print(\"Image 1 Total Keypoints: \"+str(result['img1_keypoints']))\n",
    "        print(\"Image 1 Dimensions: %s\" % (result['img1_dimension'],))\n",
    "        print(\"Image 2 Total Keypoints: \"+str(result['img2_keypoints']))\n",
    "        print(\"Image 2 Dimensions: %s\" % (result['img2_dimension'],))\n",
    "        print(\"Total Matches: \"+str(result['total_matches']))\n",
    "        print(\"Total \\\"Good\\\" Matches: \"+str(result['total_good_matches']))\n",
    "        print(\"Execution Time: \"+str(result['execution_time'])+\" seconds\")\n",
    "        print(\"----------------------------------------------\")\n",
    "    print(\"==================================\")\n",
    "    print()\n",
    "    print(\"Overall Execution Time: \"+str(execution_time)+\" seconds\")\n",
    "    print(\"=========================================\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
